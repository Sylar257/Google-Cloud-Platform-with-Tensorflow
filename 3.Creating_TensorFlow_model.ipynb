{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating_TensorFlow_model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Google-Cloud-Platform-with-Tensorflow/blob/master/Creating_TensorFlow_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqmbx2r77YtO",
        "colab_type": "text"
      },
      "source": [
        "These code suppose to be run in a GCP instance. The instructions to set up such a instance\n",
        "is documented in the *README* [file](https://github.com/Sylar257/Google-Cloud-Platform-with-Tensorflow/blob/master/READNE.md) of thie Repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PGXEJEF98Qs",
        "colab_type": "text"
      },
      "source": [
        "## Creating TensorFlow model\n",
        "In this notebook, we will be creating a tensorflow model using `tf.estimator` high-level API for our dataset on a *natality* dataset from google *BigQuery*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVrqIxSt9ZZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change these to try this notebook out\n",
        "BUCKET = 'example_bucket_26_11'      # CHANGE this to a globally unique value. Your project name is a good option to try.\n",
        "PROJECT = 'qwiklabs-gcp-00-09dd6f655043'     # CHANGE this to your project name\n",
        "REGION = 'australia-southeast1-a'    # CHANGE this to one of the regions supported by Cloud AI Platform https://cloud.google.com/ml-engine/docs/tensorflow/regions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5ljvSz192lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION\n",
        "\n",
        "# The following connects the BUCKET, PRIJECT and the workspace\n",
        "# If BUCKET do not already exist, one will be created with default settings\n",
        "%%bash\n",
        "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
        "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu7shHWy936j",
        "colab_type": "text"
      },
      "source": [
        "### Loading the data\n",
        "Use **SQL** query to access the natality data(\"LIMIT 1000\"), and create a **`Pandas` dataframe** to contain our query data.<br>\n",
        "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother. Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that -- this way, twins born on the same day won't end up in different cuts of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0QMGmOFKH5u",
        "colab_type": "code",
        "outputId": "904190f4-2303-4001-9266-c59eaa8c7f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Folllowing three lines of code allows you to run it within colab as well\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qfd1EduKTIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14zk2ZlLKSby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_id = 'qwiklabs-gcp-00-157d767039a7'\n",
        "from google.cloud import bigquery\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW0T_ZGEUsqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "680525bf-5d67-44c8-d72c-d4a6c0cfc2bf"
      },
      "source": [
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXIoqZyQU2fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine CSV, label, and key columns\n",
        "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
        "LABEL_COLUMN = 'weight_pounds'\n",
        "KEY_COLUMN = 'key'\n",
        "\n",
        "# Set default values for each CSV column\n",
        "# DEFAULTS acts as the impute value for any `nan`\n",
        "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
        "TRAIN_STEPS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRrsfiYyUsQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "a42540db-037b-4368-8d5e-1bf91358a0af"
      },
      "source": [
        "df = pd.read_csv('train.csv', names=CSV_COLUMNS)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/9e554b27bdd509f3/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 5.43659938092,\n            'f': \"5.43659938092\",\n        },\n\"True\",\n{\n            'v': 12,\n            'f': \"12\",\n        },\n\"Single(1)\",\n{\n            'v': 39.0,\n            'f': \"39.0\",\n        },\n{\n            'v': \"1451354159195218418\",\n            'f': \"1451354159195218418\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 6.49922748376,\n            'f': \"6.49922748376\",\n        },\n\"True\",\n{\n            'v': 13,\n            'f': \"13\",\n        },\n\"Single(1)\",\n{\n            'v': 34.0,\n            'f': \"34.0\",\n        },\n{\n            'v': \"524531196325542205\",\n            'f': \"524531196325542205\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 6.686620406459999,\n            'f': \"6.686620406459999\",\n        },\n\"False\",\n{\n            'v': 13,\n            'f': \"13\",\n        },\n\"Single(1)\",\n{\n            'v': 38.0,\n            'f': \"38.0\",\n        },\n{\n            'v': \"2013084202883420573\",\n            'f': \"2013084202883420573\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 7.577287944939999,\n            'f': \"7.577287944939999\",\n        },\n\"True\",\n{\n            'v': 13,\n            'f': \"13\",\n        },\n\"Single(1)\",\n{\n            'v': 40.0,\n            'f': \"40.0\",\n        },\n{\n            'v': \"1148502204931914436\",\n            'f': \"1148502204931914436\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 7.500126153239999,\n            'f': \"7.500126153239999\",\n        },\n\"True\",\n{\n            'v': 13,\n            'f': \"13\",\n        },\n\"Single(1)\",\n{\n            'v': 37.0,\n            'f': \"37.0\",\n        },\n{\n            'v': \"1148502204931914436\",\n            'f': \"1148502204931914436\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"weight_pounds\"], [\"string\", \"is_male\"], [\"number\", \"mother_age\"], [\"string\", \"plurality\"], [\"number\", \"gestation_weeks\"], [\"number\", \"key\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight_pounds</th>\n",
              "      <th>is_male</th>\n",
              "      <th>mother_age</th>\n",
              "      <th>plurality</th>\n",
              "      <th>gestation_weeks</th>\n",
              "      <th>key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.436599</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "      <td>Single(1)</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1451354159195218418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.499227</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Single(1)</td>\n",
              "      <td>34.0</td>\n",
              "      <td>524531196325542205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.686620</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>Single(1)</td>\n",
              "      <td>38.0</td>\n",
              "      <td>2013084202883420573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.577288</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Single(1)</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1148502204931914436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.500126</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Single(1)</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1148502204931914436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight_pounds is_male  ...  gestation_weeks                  key\n",
              "0       5.436599    True  ...             39.0  1451354159195218418\n",
              "1       6.499227    True  ...             34.0   524531196325542205\n",
              "2       6.686620   False  ...             38.0  2013084202883420573\n",
              "3       7.577288    True  ...             40.0  1148502204931914436\n",
              "4       7.500126    True  ...             37.0  1148502204931914436\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeddSJDmW2O0",
        "colab_type": "text"
      },
      "source": [
        "#### Create `Data_set`\n",
        "Create a object that acts as the `dataset` object in **PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmdHuNAAU8Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function reading a file using the Dataset API\n",
        "# Then provide the results to the Estimator API\n",
        "# When Data_loader() is called, it will return a function, namely _input_fn() instead of an object\n",
        "def Data_loader(filename, mode, batch_size = 512):\n",
        "  def _input_fn():\n",
        "    def decode_csv(value_column):\n",
        "      columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
        "      features = dict(zip(CSV_COLUMNS, columns))\n",
        "      label = features.pop(LABEL_COLUMN)\n",
        "      return features, label\n",
        "    \n",
        "    # Create list of files that match pattern\n",
        "    file_list = tf.gfile.Glob(filename)\n",
        "\n",
        "    # Create dataset from file list\n",
        "    dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
        "                 .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
        "      \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        num_epochs = None # indefinitely\n",
        "        dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
        "    else:\n",
        "        num_epochs = 1 # end-of-input after this\n",
        " \n",
        "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "    return dataset\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxq9RoXtNAUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define feature columns\n",
        "def get_wide_deep():\n",
        "  # Define column types\n",
        "  is_male,mother_age,plurality,gestation_weeks = \\\n",
        "      [\\\n",
        "          tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n",
        "                      ['True', 'False', 'Unknown']),\n",
        "          tf.feature_column.numeric_column('mother_age'),\n",
        "          tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n",
        "                      ['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
        "                       'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n",
        "          tf.feature_column.numeric_column('gestation_weeks')\n",
        "      ]\n",
        "\n",
        "  # Discretize\n",
        "  age_buckets = tf.feature_column.bucketized_column(mother_age, \n",
        "                      boundaries=np.arange(15,45,1).tolist())\n",
        "  gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n",
        "                      boundaries=np.arange(17,47,1).tolist())\n",
        "\n",
        "  # Sparse columns are wide, have a linear relationship with the output\n",
        "  wide = [is_male,\n",
        "          plurality,\n",
        "          age_buckets,\n",
        "          gestation_buckets]\n",
        "\n",
        "  # Feature cross all the wide columns and embed into a lower dimension\n",
        "  crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n",
        "  embed = tf.feature_column.embedding_column(crossed, 3)\n",
        "\n",
        "  # Continuous columns are deep, have a complex relationship with the output\n",
        "  deep = [mother_age,\n",
        "          gestation_weeks,\n",
        "          embed]\n",
        "  return wide, deep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHBnWDdGXGGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create serving input function to be able to serve predictions later using provided inputs\n",
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "        'is_male': tf.placeholder(tf.string, [None]),\n",
        "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
        "        'plurality': tf.placeholder(tf.string, [None]),\n",
        "        'gestation_weeks': tf.placeholder(tf.float32, [None])\n",
        "    }\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1)\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsyFsot0XLOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create estimator to train and evaluate\n",
        "def train_and_evaluate(output_dir):\n",
        "  wide, deep = get_wide_deep()\n",
        "  EVAL_INTERVAL = 300\n",
        "  run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
        "                                      keep_checkpoint_max = 3)\n",
        "  estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
        "                       model_dir = output_dir,\n",
        "                       linear_feature_columns = wide,\n",
        "                       dnn_feature_columns = deep,\n",
        "                       dnn_hidden_units = [64, 32],\n",
        "                       config = run_config)\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "                       input_fn = Data_loader('train.csv', mode = tf.estimator.ModeKeys.TRAIN),\n",
        "                       max_steps = TRAIN_STEPS)\n",
        "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "                       input_fn = Data_loader('eval.csv', mode = tf.estimator.ModeKeys.EVAL),\n",
        "                       steps = None,\n",
        "                       start_delay_secs = 60, # start evaluating after N seconds\n",
        "                       throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
        "                       exporters = exporter)\n",
        "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_XIBYuuXJ0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55e063db-a25b-465b-d658-9cfca72e4b0d"
      },
      "source": [
        "# Run the model\n",
        "shutil.rmtree('babyweight_trained', ignore_errors = True) # start fresh each time\n",
        "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
        "train_and_evaluate('babyweight_trained')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'babyweight_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f656aa8bdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
            "\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3079: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:305: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 225047.69, step = 1\n",
            "INFO:tensorflow:global_step/sec: 37.8098\n",
            "INFO:tensorflow:loss = 8589.492, step = 101 (2.646 sec)\n",
            "INFO:tensorflow:global_step/sec: 40.8125\n",
            "INFO:tensorflow:loss = 2596.4146, step = 201 (2.453 sec)\n",
            "INFO:tensorflow:global_step/sec: 40.7563\n",
            "INFO:tensorflow:loss = 2143.8784, step = 301 (2.455 sec)\n",
            "INFO:tensorflow:global_step/sec: 41.348\n",
            "INFO:tensorflow:loss = 1348.3499, step = 401 (2.419 sec)\n",
            "INFO:tensorflow:global_step/sec: 40.172\n",
            "INFO:tensorflow:loss = 1045.7695, step = 501 (2.485 sec)\n",
            "INFO:tensorflow:global_step/sec: 40.389\n",
            "INFO:tensorflow:loss = 1147.923, step = 601 (2.476 sec)\n",
            "INFO:tensorflow:global_step/sec: 39.7757\n",
            "INFO:tensorflow:loss = 977.9648, step = 701 (2.514 sec)\n",
            "INFO:tensorflow:global_step/sec: 40.0557\n",
            "INFO:tensorflow:loss = 1056.8274, step = 801 (2.496 sec)\n",
            "INFO:tensorflow:global_step/sec: 39.7927\n",
            "INFO:tensorflow:loss = 922.97736, step = 901 (2.516 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into babyweight_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-11-28T00:16:45Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-11-28-00:16:46\n",
            "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.4841466, global_step = 1000, label/mean = 7.2368712, loss = 742.98663, prediction/mean = 7.117276\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: babyweight_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
            "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
            "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
            "WARNING:tensorflow:Export includes no default signature!\n",
            "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: babyweight_trained/export/exporter/temp-b'1574900206'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 714.00024.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfNKW44HXZMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
